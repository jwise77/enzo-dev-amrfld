\documentclass[letterpaper,10pt]{article}
\usepackage{geometry}   % See geometry.pdf to learn the layout
                        % options.  There are lots.
\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath,amsfonts,amssymb}

\usepackage{thumbpdf}   
\usepackage[usenames,pdftex]{color} % black,white,red,green,blue,cyan,magenta,yellow
\pdfcompresslevel=1  
\def\myfig#1#2{\pdfimage height #2 {#1.pdf}}
\def\myfigpdf#1#2{\pdfimage height #2 {#1.pdf}}
\def\myfigpng#1#2{\pdfimage height #2 {#1.png}}

\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\newcommand{\vb}{{\bf v}_b}
\newcommand{\xvec}{{\bf x}}
\newcommand{\Omegabar}{\bar{\Omega}}
\newcommand{\rhob}{\rho_b}
\newcommand{\dt}{\Delta t}
\newcommand{\Eot}{E^{OT}}
\newcommand{\Ef}{E_f}
\newcommand{\sighat}{\hat{\sigma}}
\newcommand{\Fnu}{{\bf F}_{\nu}}
\newcommand{\Pnu}{\overline{\bf P}_{\nu}}
\newcommand{\R}{I\!\!R}
\newcommand{\Rthree}{\R^3}
\newcommand{\eh}{e_h}
\newcommand{\ec}{e_c}
\newcommand{\Edd}{\mathcal F}
\newcommand{\Eddnu}{\Edd_{\nu}}
\newcommand{\mn}{{\tt n}}
\newcommand{\mB}{\mathcal B}
\newcommand{\mC}{{\mathcal C}}
\newcommand{\mL}{{\mathcal L}}
\newcommand{\mD}{{\mathcal D}}
\newcommand{\mDnu}{\mD_{\nu}}
\newcommand{\mCnu}{\mC_{\nu}}
\newcommand{\mLnu}{{\mathcal L}_{\nu}}
\newcommand{\mCe}{\mC_e}
\newcommand{\mLe}{\mL_e}
\newcommand{\mCn}{\mC_{\mn}}
\newcommand{\mLn}{\mL_{\mn}}
\newcommand{\amrsolve}{{\tt AMRSolve} }
\newcommand{\enzo}{{\tt Enzo} }
\newcommand{\hypre}{{\tt HYPRE} }
\newcommand{\cpp}{{\tt C++} }


\textheight 9truein
\textwidth 6.5truein
\addtolength{\oddsidemargin}{-0.25in}
\addtolength{\evensidemargin}{-0.25in}
\addtolength{\topmargin}{-0.5in}
\setlength{\parindent}{0em}
\setlength{\parskip}{2ex}


\author{Daniel R. Reynolds and James O. Bordner}
\title{\amrsolve User Guide}


\begin{document}
\maketitle

\section{Introduction}
\label{sec:intro}

The \amrsolve package is an \enzo add-on that strives to achieve two
goals: (a) provide a smoother approach for iteration over grids in an
\enzo AMR hierarchy, both within and between levels, and within and
between MPI tasks, and (b) to provide a straightforward interface for
defining linear systems on block-structured adaptive mesh hierarchies
and solving them using iterative linear solvers provided by the \hypre
solver library.  The first of these alone allows \amrsolve to
significantly benefit \enzo development of new physics modules that
require inter-grid communication, if only due to the ease of
development.  However, the combination of the two goals of \amrsolve
has allowed us to construct two \enzo solvers for implicit systems
defined on AMR grids: self-gravity and flux-limited-diffusion
radiation transport.

In this user guide, we first provide a brief overview of the \amrsolve
package structure, including a description of its suite of \cpp
classes and iterators.  We then focus on the self-gravity solver, 
{\tt AMRGravitySolve}. We will conclude with a discussion of the FLD
radiation transport solver, {\tt AMRFLDSplit}. 


\section{\amrsolve}
\label{sec:amrsolve}

The \amrsolve package contains a relatively large number of classes,
providing a range of utilities from description of the \enzo scalar
type to MPI communicator wrappers, to HYPRE solver interfaces.  All
\amrsolve package routines are held in the \enzo directory 
{\tt src/amr\_solve/}, which are symbolically linked during \enzo
compilation to the {\tt src/enzo/} directory.  We discuss each of
these \cpp classes below, focusing more on those utilities that
interface most closely with \enzo.
\begin{itemize}
\item {\tt AMRsolve\_Domain} -- small class containing the problem
  dimensionality and root-grid extents.
\item {\tt AMRsolve\_Hierarchy} -- this class represents {\tt Enzo}'s
  AMR hierarchy.  However, unlike the \enzo hierarchy structures, this
  class comes with friend classes that provide an invaluable set of
  iterators throughout the hierarchy:
  \begin{itemize}
  \item {\tt ItHierarchyGridsLocal} -- iterator class for visiting all
    task-local grids within an \\
    {\tt AMRsolve\_Hierarchy} object.
  \item {\tt ItHierarchyGridsAll} -- iterator class for visiting all
    grids within an {\tt AMRsolve\_Hierarchy} object (akin to the
    linked-list iteration type through the \enzo hierarchy).
  \item {\tt ItHierarchyLevels} -- iterator class for visiting all
    levels in an an {\tt AMRsolve\_Hierarchy} object, running from
    coarsest to finest.
  \end{itemize}
  An {\tt AMRsolve\_Hierarchy} object is set up within \enzo by
  ``attaching'' it to an \enzo {\tt LevelArray}.  There are currently
  two such routines, one for gravity problems 
  ({\tt enzo\_attach\_grav}) and the other for radiation problems 
  ({\tt enzo\_attach\_fld}).  Upon solution of the necessary
  gravity/radiation problem, the {\tt AMRsolve\_Hierarchy} object may
  be detached from enzo via the {\tt enzo\_detach} routine, allowing
  for adaptivity of the mesh before re-attachment.
\item {\tt AMRsolve\_Level} -- small class for encapsulating a single
  level of an AMR hierarchy.  Again, the true utility in this class is
  it's friend class iterators:
  \begin{itemize}
  \item {\tt ItLevelGridsLocal} -- iterator class for visiting all
    task-local grids in an {\tt AMRsolve\_Level} object.
  \item {\tt ItLevelGridsAll} -- iterator class for visiting all
    grids in an {\tt AMRsolve\_Level} object.
  \end{itemize}
\item {\tt AMRsolve\_Grid} -- class for representing a grid patch in
  an AMR hierarchy.  Each grid is an orthogonal grid of zones in d
  dimensions.  These grids contain a small amount of data for
  interfacing with \hypre solvers, as well as descriptors of the
  corresponding \enzo grid object.  However, these grids do store
  pointers to some of the corresponding \enzo data fields that are
  used in solving up gravity and radiation problems.  The true benefit
  of this class is not to merely reproduce a descriptor of each \enzo
  grid object; instead it comes from the suite of associated
  iterators, that obviate the need to iterate over a full hierarchy to
  determine inter-grid relationships:
  \begin{itemize}
  \item {\tt id\_parent\_} -- this member points to the parent {\tt
    AMRsolve\_Grid}.
  \item {\tt ItGridNeighbors} -- this class iterates over all
    neighboring grids of an {\tt AMRsolve\_Grid}.
  \item {\tt ItGridChildren} -- this class iterates over all child
    grids of an {\tt AMRsolve\_Grid}.
  \end{itemize}
\item {\tt AMRsolve\_Faces} -- each {\tt AMRsolve\_Grid} object has a
  corresponding {\tt AMRsolve\_Faces} object, that identifies what the
  boundaries of the grid are adjacent to (e.g.~coarser cells, finer
  cells, external boundary).
\item {\tt AMRsolve\_Mpi} -- small wrapper class containing the MPI
  communicator, communicator size, and MPI task index.
\item {\tt AMRsolve\_Parameters} -- small class for storing and
  accessing run-time parameters using key-value pairs.
\item {\tt AMRsolve\_Point} -- small class for representing point
  masses (used in some self-gravity tests).
\item {\tt AMRsolve\_Problem} -- small class for setting up
  self-gravity test problems.
\item {\tt AMRsolve\_scalar.h} -- file to define a 'Scalar' type (as
  opposed to re-defining 'float').
\item {\tt AMRsolve\_Hypre\_Grav} -- class for interfacing between the
  \enzo routine {\tt AMRGravitySolve} and \hypre, to perform scalable,
  self-consistent self-gravity solves over a full AMR hierarchy.  This
  will be discussed in further detail in Section
  \ref{sec:AMRGravitySolve}.
\item {\tt AMRsolve\_Hypre\_FLD} -- class for interfacing between the
  \enzo class {\tt AMRFLDSplit} and \hypre, to perform scalable,
  self-consistent flux-limited-diffusion radiation transport solves
  over a full AMR hierarchy.  This will be discussed in further detail
  in Section \ref{sec:AMRFLDSplit}.
\item {\tt AMRsolve\_HG\_Prec} -- class to provide a
  ``hierarchical-grid'' preconditioner for fully implicit linear
  systems posed on block-structured AMR meshes.  This preconditioner
  is designed to be plugged into a \hypre iterative linear solver,
  such as BiCGStab or GMRES, and aids in constructing a scalable
  solver for elliptic (gravity) or parabolic (FLD) linear systems.
\end{itemize}



\section{The {\tt AMRGravitySolve} Routine}
\label{sec:AMRGravitySolve}

The {\tt AMRGravitySolve} routine provides an \enzo interface to the
{\tt AMRsolve\_Hypre\_Grav} class within {\tt AMRSolve}.  As with the
existing split gravity solver ({\tt CommunicationSolvePotential})
within \enzo, {\tt AMRGravitySolve} is called within 
{\tt PrepareDensityField}.  The choice between these two solvers is
made through the \\ 
{\tt SelfGravityConsistent} input parameter (0
implies {\tt CommunicationSolvePotential} and nonzero implies 
{\tt AMRGravitySolve}, with 0 the default).  In order to use this
solver, both of the compile-time options {\tt AMR\_SOLVE} and 
{\tt USE\_HYPRE} must be enabled.  

Assumed code fields:
\begin{itemize}
\item PotntialField
\item GravitatingMassField
\end{itemize}

When using {\tt AMRGravitySolve}, the self-gravity equation
\begin{equation}
\label{eq:self_gravity}
  \nabla^2 \phi = \frac{4\pi G}{a}(\rho_b + \rho_{dm} + <\!\!\rho_0\!\!>)
\end{equation}
is set up and solved over the AMR hierarchy defined from the root grid
level to the level input to the {\tt PrepareDensityField} function.
Since {\tt PrepareDensityField} is called within {\tt EvolveLevel},
i.e.~on each level in the AMR hierarchy, then {\tt AMRGravitySolve}
will be called on successively deeper or shallower AMR hierarchies as
the W-cycle through the levels is traversed.  The Laplacian is
discretized using a standard second-order accurate finite difference
approximation within each grid patch, with piecewise-constant
interpolation to bridge coarse-fine interface boundaries.

In solving the system \eqref{eq:self_gravity}, {\tt AMRGravitySolve}
will use one of three solver algorithms: GMRES, BiCGStab, or FAC.  The
first two of these are Krylov iterative solver algorithms, while the
third is a multigrid variant called the ``Fast Adaptive Composite''
method \cite{McCormick1989}.  The choice of solver algorithm is
determined by the input parameter {\tt AMRGravitySolve\_solver}, where
1 corresponds to GMRES, 2 to FAC, and otherwise to BiCGStab (0 is the
default).  Of these three methods, only BiCGStab and GMRES are
recommended, as the FAC algorithm seems less robust.

As all of these linear solvers are iterative, a maximum iteration
count may be applied through the input parameter 
{\tt AMRGravitySolve\_maxit}, that has default value 200.

When using either of the Krylov linear solvers, the linear system may
be preconditioned using the multigrid-like hierarchical grid
preconditioner defined by the \amrsolve class {\tt AMRsolve\_HG\_Prec}
mentioned above.  This is enabled with the input parameter 
{\tt AMRGravitySolve\_useprec}, where 0 disables the preconditioner
and all other options enable it (1 is the default).  When using this
preconditioner, a number of other options may be specified:
\begin{itemize}
\item As the HG preconditioner may also be applied iteratively, a
  maximum number of iterations may be specified through the parameter
  {\tt AMRGravitySolve\_precmaxit} (defaults to 1).
\item Similarly, a linear-residual-based stopping criteria for the HG
  solve may be specified through the parameter {\tt
  AMRGravitySolve\_restol} (defaults to 0.0, i.e.~disabled).
\item The internal number of Jacobi smoothing iterations that HG
  applies to the full AMR hierarchy may be specified through the input
  parameter {\tt AMRGravitySolve\_Jaciters} (defaults to 3).
\item The inner \hypre geometric multigrid solver may be controlled
  through the input parameters \\
  {\tt AMRGravitySolve\_rlxtype} and  
  {\tt AMRGravitySolve\_npre}, that specify the relaxation type
  (default is 2, i.e.~symmetric red/black Gauss-Seidel) and the number
  of pre- and post-smoothing iterations (default is 2).
\end{itemize}


\subsection{{\tt AMRGravitySolve} Results}
\label{sec:AMRGravitySolve_results}

We have investigated the {\tt AMRGravitySolve} solver for both
accuracy and efficiency using a modification of the built-in \enzo
test {\tt AMRGravityTest}.  This test problem is constructed using a
dark-matter only simulation, consisting of a single very massive
particle near the center of the computational domain, surrounded by a
large number of very light, randomly placed particles.  The potential
field is then calculated using either of the built-in \enzo gravity
solvers, and the gravitational acceleration is calculated on each
particle and compared against the semi-analytical radial and
tangential forces that should arise from a $r^{-2}$ potential law.
Plots of results computed using both the original FFT-based gravity
solver and the {\tt AMRGravitySolve} solver are shown in Figure
\ref{fig:AMRGravityTest_results}. 

\begin{figure}[h]
\[
  \centerline{\myfigpng{GravityTest_orig}{2.5in}
    \myfigpng{GravityTest_new}{2.5in} }
\]
\caption{Test results using original (left) gravity solver and 
  {\tt AMRGravitySolve} (right).  Both tests use 10000 particles on a
  $64^3$ grid.  The RMS error for both tests is
  0.0755 and 0.0732, respectively, and appears to arise from reduced
  error at the coarse-fine grid boundary seen at approximately $r=4$.} 
\label{fig:AMRGravityTest_results}
\end{figure}

Similarly, we have constructed a parallel version of this test for
investigation of weak scaling, entitled 
{\tt AMRGravity\_weak\_scaling}.  This test is similar to 
{\tt AMRGravityTest}, but here each processor initializes a single
heavy particle and a large number of randomly-placed light particles
within it's root-grid processor tile.  While the test problem no
longer admits a simple semi-analytical solution, it does allow
parallel tests with an AMR hierarchy of a specified depth.  The test
does not initialize a static hierarchy, but instead initializes only
the root grid and enables adaptive mesh refinement.  We use a
refinement factor of 2 between levels, and refine at each level
geometrically to ensure that each refinement level contains the same
total number of cells as its parent level.  We ran tests of this
solver using AMR depths ranging from 1 through 9 levels, and processor
counts $p=k^3, k=1,\ldots,7$ (from 1 to 343 processors).  Each test
was performed for 8 root-grid time steps, to ensure that AMR
refinement had proceeded fully to the finest level, and timing
statistics were taken for only the final gravity solve in this time
step.  These tests were performed using both an un-preconditioned
BiCGStab linear solver for the gravity system, and a BiCGStab solver
preconditioned using the {\tt AMRsolve\_HG\_Prec} class (HG prec.).
Unfortunately, the un-preconditioned solver was unable to converge
within a maximum allowed 5000 iterations for the two deepest
hierarchies (8 and 9 levels, respectively), so results from those
tests are omitted.

In figure \ref{fig:AMRGravity_weak_scaling_results1} we plot the total
solution time versus processor count for each AMR depth, to
demonstrate the weak-scaling performance of the solvers.  As is
clearly evident in these plots, after an expected increase in solution
time when going from 1 to 8 processes, the total solution time for
each hierarchy depth remains relatively flat, increasing at the
optimal $O(log p)$ rate indicative of multigrid solvers, with the
HG-preconditioned results uniformly outperforming the
un-preconditioned solver.

\begin{figure}[h]
\[
  \centerline{\myfigpdf{scaling_np}{2.5in}
    \myfigpdf{scaling_p}{2.5in} }
\]
\caption{Weak scaling results using an un-preconditioned (left) and
  HG-preconditioned (right) gravity solver.  Curves are shown for
  differening AMR hierarchy depths, and indicate near-optimal weak
  scaling as the parallelism is increased, after an expected run-time
  increase when transitioning from serial (1 process) to parallel (8
  processes).  Additionally, the benefit from preconditioning is
  clearly evident, with curves on the right plot uniformly lower than
  corresponding curves on the left.} 
\label{fig:AMRGravity_weak_scaling_results1}
\end{figure}

Similarly, in figure \ref{fig:AMRGravity_weak_scaling_results2} we
plot the average solution time per computational cell as levels are
added to the AMR hierarchy, for a variety of different processor
counts.  The solution time increase when going from 1 to 2 levels may
be easily understood due to the sudden increase in computational
infrastructure required when going from a uniform to an adaptive
mesh.  Moreover, in these plots the benefit of the HG preconditioner
is more obvious due to the significantly flatter curves in the plot at
the right.  However, it is also clear that the efficacy of the HG
preconditioner reduces for deeper AMR hierarchies, seen here through
the increase when going deeper than 7 levels of refinement.  From
these plots, it is evident that additional research is needed to
construct a preconditioner for deep AMR runs.

\begin{figure}[h]
\[
  \centerline{\myfigpdf{timing_np}{2.5in}
    \myfigpdf{timing_p}{2.5in} }
\]
\caption{Results showing average solution per computational cell as
  the mesh is refined.  The un-preconditioned BiCGStab solver is shown
  at left, and the HG-preconditioned solver at right, with curves
  shown for runs using differing processor counts.  Note the much
  flatter curves when using the HG-preconditioner, at least through 7
  levels of mesh refinement, notwithstanding the expected increase
  when transitioning from unigrid (1 level) to AMR (2 levels).  Note:
  without preconditioning the solver could no longer converge within a
  maximum allowed 5000 iterations when using 8- and 9-level hierarchies.}
\label{fig:AMRGravity_weak_scaling_results2}
\end{figure}





\section{The {\tt AMRFLDSplit} Class}
\label{sec:AMRFLDSplit}

As with the {\tt AMRGravitySolve} routine, the {\tt AMRFLDSplit}
class implements an \enzo interface to the \\
{\tt AMRsolve\_Hypre\_FLD} class.  As with the variety of other
radiation transport solvers within \enzo, {\tt AMRFLDSplit} is called
near the beginning of {\tt EvolveLevel}, and is enabled through the
{\tt TRANSFER} compile-time option.  Also, as with 
{\tt AMRGravitySolve}, the compile-time options {\tt AMR\_SOLVE} and
{\tt USE\_HYPRE} are required.

Assumed code fields:
\begin{itemize}
\item RadiationFreq0
\item Emissivity0
\item kphHI
\item kphHeI
\item kphHeII
\item PhotoGamma
\item kdissH2I
\item HIDensity
\item HeIDensity
\item HeIIDensity
\end{itemize}




\subsection{Input Parameters}
\label{sec:AMRFLDSplit_parameters}

Main \enzo parameter file:
\begin{itemize}
\item {\tt RadiativeTransferFLD} = 2
\item {\tt ImplicitProblem} = 6
\item {\tt RadiativeTransferOpticallyThinH2} = 0
\item {\tt MultiSpecies}
\item {\tt RadiativeCooling}
\item {\tt RadHydroParamfile}
\end{itemize}

FLD-specific parameter file:
\begin{itemize}
\item {\tt RadHydroESpectrum}
\item {\tt RadHydroModel}
\item {\tt RadHydroMaxDt}
\item {\tt RadHydroMinDt}
\item {\tt RadHydroInitDt}
\item {\tt RadHydroDtRadFac}
\item {\tt RadHydroSolType}
\item {\tt RadHydroMaxMGIters}
\item {\tt RadHydroMGPreRelax}
\item {\tt RadHydroMGPostRelax}
\end{itemize}




\subsection{Flux-limited diffusion radiation model}
\label{sec:rad_model}

We begin with the equation for flux-limited diffusive radiative
transfer in a cosmological medium \cite{ReynoldsHayesPaschosNorman2009},
\begin{equation}
\label{eq:radiation_PDE}
  \partial_{t} E + \frac1a \nabla\cdot\(E\vb\) =
    \nabla\cdot\(D\,\nabla E\) - \frac{\dot{a}}{a} E - c\kappa E + \eta,
\end{equation}
where here the comoving radiation energy density $E$, emissivity
$\eta$ and opacity $\kappa$ are functions of space and time.  In this
equation, the frequency-dependence of the radiation energy has been
integrated out, under the premise of an assumed radiation energy
spectrum, 
\begin{align}
  \notag
  & E_{\nu}(\nu,\xvec,t) = \tilde{E}(\xvec,t) \chi(\nu), \\
  \notag
  \Rightarrow & \\
  \label{eq:spectrum}
  & E(\xvec,t) = \int_{\nu_{HI}}^{\infty} E_{\nu}(\nu,\xvec,t)\,\mathrm{d}\nu 
    = \tilde{E}(\xvec,t) \int_{\nu_{HI}}^{\infty} \chi(\nu)\,\mathrm{d}\nu,
\end{align}
where $\tilde{E}$ is an intermediate quantity (for analysis) that is
never computed.  We note that if the assumed spectrum is the Dirac
delta function, $\chi(\nu) = \delta_{\nu_{HI}}(\nu)$, $E$ is a
monochromatic radiation energy density at the ionization threshold of
HI, and the $-\frac{\dot{a}}{a}E$ term (obtained through integration
by parts of the redshift term
$\frac{\dot{a}}{a}\partial_{\nu}E_{\nu}$) is omitted from
\eqref{eq:radiation_PDE}. Similarly, the emissivity function
$\eta(\xvec,t)$ relates to the true emissivity 
$\eta_{\nu}(\nu,\xvec,t)$ by the formula
\begin{equation}
\label{eq:emissivity}
  \eta(\xvec,t) = \int_{\nu_{HI}}^{\infty}\eta_{\nu}(\nu,\xvec,t)\,\mathrm{d}\nu.
\end{equation}

The function $D$ in the above equation \eqref{eq:radiation_PDE} is
the {\em flux-limiter} that depends on $E$, $\nabla E$ and the 
opacity $\kappa$,  
\[
   D(E) = \text{diag}\( D_1(E),\, D_2(E),\, D_3(E) \),
\]
where the directional limiters $D_i(E)$ are given by \cite{Morel2000}
\begin{align}
  \label{eq:Larsen_limiter}
   D_i(E) = \frac{c}{\sqrt{(3\kappa)^2 + R_i^2}}, \qquad
   R_i(E) = \max\left\{\frac{|\partial_i E|}{E}, 10^{-20} \right\}.
\end{align}



\subsection{Radiation Spectrum}
\label{sec:AMRFLDSplit_spectrum}



\subsection{Emissivity}
\label{sec:AMRFLDSplit_emissivity}



\subsection{Opacity}
\label{sec:AMRFLDSplit_opacity}



\subsection{Chemical Ionization and Gas Cooling}
\label{sec:AMRFLDSplit_chemistry}

In general, radiation calculations in Enzo are used in simulations
where chemical ionization states are important.  For these situations, 
we couple the radiation equation \eqref{eq:radiation_PDE} with
equations for both the specific gas energy correction and the
ionization dynamics of Hydrogen,
\begin{align}
  \notag
  \partial_t e_c &= -\frac{2\dot{a}}{a}e_c + G - \Lambda, \\
  \label{eq:hydrogen_ionization}
  \partial_t \mn_{HI} + \frac{1}{a}\nabla\cdot\(\mn_{HI}\vb\) &=
    \alpha^{rec} \mn_e \mn_{HII} - \mn_{HI} \Gamma_{HI}^{ph}. 
\end{align}
Here, $\mn_{HI}$ is the comoving Hydrogen I number density.  The
recombination rate $\alpha^{rec}$ is given by the case-B
recombination rate, 
\begin{equation}
\label{eq:alphaB}
\alpha^{rec} = 2.753\times 10^{-14} \left(\frac{3.15614\times 10^5}{T}\right)^{3/2} 
                   \left(1+\left(\frac{3.15614\times 10^5}{2.74\, T}\right)^{0.407}\right)^{-2.242}.
\end{equation}

In this model, the gas heating and cooling rates are
chemistry-dependent, 
\begin{align}
  \label{eq:G_nLTE}
  G &= \frac{c\,E\,\mn_{HI}}{\rhob} 
    \left[\int_{\nu_{HI}}^{\infty} \sigma_{HI}\, \chi_E
    \left(1-\frac{\nu_{HI}}{\nu}\right)\, d\nu\right] \bigg/
    \left[\int_{\nu_{HI}}^{\infty} \chi_E d\nu\right], \\
\label{eq:Lambda_nLTE}
  \Lambda &= \frac{\mn_e}{\rhob}\bigg[\text{ce}_{HI}\, \mn_{HI} 
  + \text{ci}_{HI}\, \mn_{HI} + \text{re}_{HII}\, \mn_{HII} + \text{brem}\,
  \mn_{HII} \\
  \notag &\qquad+ \frac{m_h}{\rho_{units}\, a^3} \left(\text{comp}_1\, (T-\text{comp}_2) 
    + \text{comp}_{X}\, (T-\text{comp}_{T})\right) \bigg].
\end{align}
The temperature-dependent cooling rates
$\text{ce}_{HI}$, $\text{ci}_{HI}$, $\text{re}_{HII}$, $\text{brem}$,
$\text{comp}_1$, $\text{comp}_2$, $\text{comp}_{X}$ and
$\text{comp}_{T}$ are all taken from Enzo's built-in rate tables.

Moreover, the frequency-integrated opacity is now chemistry-dependent,
\begin{equation}
\label{eq:opacityHI}
  \kappa \ = \ 
  \left[\int_{\nu_{HI}}^{\infty} \kappa_{\nu}\,E_{\nu}\,d\nu\right] \bigg/
  \left[\int_{\nu_{HI}}^{\infty} E_{\nu}\,d\nu\right] \ = \ 
  \left[\mn_{HI} \int_{\nu_{HI}}^{\infty}
    \chi_E\,\sigma_{HI}\,d\nu\right] \bigg/
  \left[\int_{\nu_{HI}}^{\infty} \chi_E\,d\nu\right],
\end{equation}
where these integrals with the assumed radiation spectrum $\chi(\nu)$
handle the change from the original frequency-dependent radiation
equation to the integrated grey radiation equation.



\subsection{Interface to external Enzo chemistry/cooling solvers}
\label{sec:chem_model_external}

If {\tt AMRFLDSplit} is not used to evolve equations
\eqref{eq:cons_energy_correction} or \eqref{eq:hydrogen_ionization},
then {\tt AMRFLDSplit} will instead fill in the spatially-dependent
coupling terms $G$, $\Gamma_{HI}^{ph}$ and possibly
$\Gamma_{HeI}^{ph}$ and $\Gamma_{HeII}^{ph}$, for use by external
ionization/heating solvers.  These are held in the baryon fields 
{\tt PhotoGamma}, {\tt kphHI}, {\tt kphHeI} and {\tt kphHeII},
respectively.  We note that since the radiation fields evolved by 
{\tt AMRFLDSplit} do not include frequencies below $\nu_{HI}$, the
baryon field {\tt kdissH2I} is always set to 0.



\subsection{Numerical Solution Approach}
\label{sec:AMRFLDSplit_rad_solve}

We discretize the radiation equation \eqref{eq:radiation_PDE} using a
standard two-level $\theta$-method,
\begin{align}
  \label{eq:radiation_PDE_theta}
  E^n - E^{n-1} &- \theta\dt\left(\nabla\cdot\(D\,\nabla E^n\) - \frac{\dot{a}}{a} E^n -
    c\kappa^n E^n + \eta^n\right) \\ 
  \notag
  & - (1-\theta)\dt\left(\nabla\cdot\(D\,\nabla E^{n-1}\) - \frac{\dot{a}}{a} E^{n-1} -
    c\kappa^{n-1} E^{n-1} + \eta^{n-1}\right) = 0,
\end{align}
where $0\le\theta\le 1$ defines the time-discretization, and where we
have assumed that the advective portion of \eqref{eq:radiation_PDE}
has already been taken care of through Enzo's hydrodynamics routines
via color field advection.  Recommended values of $\theta$ are 1
(backwards Euler) and $\frac12$ (trapezoidal, a.k.a.~Crank-Nicolson).

Whichever $\theta$ value we use (as long as it is nonzero), the
equation \eqref{eq:radiation_PDE_theta} is linearly-implicit in the
time-evolved radiation energy density $E^n$.  We write this in
predictor-corrector form, which we will write as
\begin{align}
\label{eq:linear_system}
  J s = b, \qquad E^n = E^{n-1} + s.
\end{align}
We approximately solve this linear equation for the update $s$,
to a tolerance $\delta$,
\begin{align}
\label{eq:linear_system_approx}
  \| J s - b \|_2 \le \delta,
\end{align}
using one out of the same set of solvers as available to the 
{\tt AMRGravitySolve} routine.




\subsection{Time-step selection}
\label{sec:AMRFLDSplit_dt_selection}

Time steps are chosen adaptively in an attempt to control error in the
calculated solution.  To this end, we first define an heuristic
measure of the time accuracy error in a specific variable $u$ as
\begin{align}
\label{eq:time_error}
  err = \left(\frac1N \sum_{i=1}^N
    \left(\frac{u_i^{n}-u_i^{n-1}}{\omega_i}\right)^p\right)^{1/p}, 
\end{align}
where the weighting vector $\omega$ is given by
\begin{align}
\label{eq:time_weighting}
  \omega_i &= \sqrt{u_i^n u_i^{n-1}} + 10^{-3}, \quad i=1,\ldots,N, \\
  \omega_i &= |e_{c,i} + e_{h,i}| + 10^{-3}, \quad i=1,\ldots,N,
\end{align}
i.e.~we scale the radiation and chemistry change by the geometric mean
of the old and new states, and scale the gas energy change by the new
total gas energy, adding on a floor value of $10^{-3}$ in case any
of the states are too close to zero.  This approach works well when
the internal solution variables are unit-normalized, or at least close
to unit-normalized, since the difference between the old and new
solutions, divided by this weighting factor $\omega$, should give a
reasonable estimate of the number of significant digits that are
correct in the solution. 

With these error estimates \eqref{eq:time_error} for each variable, we
set the new time step size for each subsystem based on the previous
time step size and a user-input tolerance $\tau_{\text{tol}}$ as
\begin{align}
\label{eq:time_estimate}
  \dt^{n} = \frac{\tau_{\text{tol}} \dt^{n-1}}{err}.
\end{align}
Since $E$ and $\{e_c,\mn_{HI}\}$ are evolved separately,
we allow the $\{e_c,\mn_{HI}\}$ solver to subcycle at a faster rate
than the $E$.  We therefore have two time step sizes that we use in
the module,
\begin{align}
\label{eq:FLD_time_estimate}
  \dt_{E}^{n} &= \min\{\dt_{E}^{n},\dt_{CFL}^{n}\}. \\
  \dt_{e,HI}^{n} &= \min\{\dt_{e}^{n},\dt_{HI}^{n},\dt_{E}^{n}\},
\end{align}
where $\dt_{\text{CFL}}$ is the time step size that Enzo's other
routines (e.g.~hydrodynamics) would normally take.  A user may
override these adaptive time step controls with the input parameters 
$\dt_{\text{max}}$ and $\dt_{\text{min}}$. 

We further note that when run in combination with Enzo's hydrodynamics
routines, both modules will limit their maximum time step sizes to the
minimum of $\dt_{\text{E}}$ and $\dt_{\text{CFL}}$.  As a result, in
some physical regimes, the global time step size will be limited based
on the radiation time scale, and in other regimes it will be limited
by the hydrodynamic time scale. 



\subsection{Variable rescaling}
\label{sec:AMRFLDSplit_variable_rescaling}

In case Enzo's standard unit non-dimensionalization using 
{\tt DensityUnits}, {\tt LengthUnits} and {\tt TimeUnits} is
insufficient to render the resulting solver values $E$, $e_c$ and
$n_{HI}$ to have nearly unit magnitude, the user may input additional
variable scaling factors to be used inside the {\tt AMRFLDSplit}
module.  Denoting these user-input values as $s_E$, $s_e$ and
$s_{\mn}$, then we may define the rescaled variables
\begin{align}
\label{eq:variable_rescaling}
  \tilde{E} = E / s_E, \qquad \tilde{e}_c = e_c / s_e, \qquad 
  \tilde{\mn}_{HI} = \mn_{HI} / s_{\mn},
\end{align}
and the {\tt AMRFLDSplit} module will use $\tilde{E}$, $\tilde{e}_c$ and
$\tilde{\mn}_{HI}$ in its internal routines instead of Enzo's internal
variables $E$, $e_c$ and $\mn_{HI}$.  If the user does not know
appropriate values for these scaling factors {\em a-priori}, a
generally-applicable rule of thumb is to first run their simulation
for a small number of time steps and investigate Enzo's HDF5 output
files to see the magnitude of the values stored internally by Enzo; if
these are far from unit-magnitude, these scaling factors should be
used. 



\subsection{Boundary conditions}
\label{sec:AMRFLDSplit_boundary_conditions}

As the radiation equation \eqref{eq:radiation_PDE} is parabolic,
boundary conditions must be supplied on the radiation field $E$.  The
{\tt AMRFLDSplit} module allows three types of boundary conditions to
be placed on the radiation field:
\begin{itemize}
\item[0.] Periodic,
\item[1.] Dirichlet, i.e.~$E(x,t) = g(x), \; x\in\partial\Omega$, and
\item[2.] Neumann, i.e.~$\nabla E(x,t)\cdot n = g(x), \; x\in\partial\Omega$.
\end{itemize}
In most cases, the boundary condition types (and values of $g$) are
problem-dependent.  When adding new problem types, these conditions
should be set near the bottom of the file {\tt AMRFLDSplit\_Initialize.C}, 
otherwise these will default to either (a) periodic, or (b) will use
$g=0$, depending on the user input boundary condition type.



\subsection{Refinement Strategies}
\label{sec:AMRFLDSplit_refinement}

\begin{itemize}
\item {\tt Grid\_FlagCellsToBeRefinedByIonizedFraction.C}
\item {\tt Grid\_FlagCellsToBeRefinedByOpacity.C}
\item {\tt Grid\_FlagCellsToBeRefinedByRadiationGradient.C}
\item {\tt Grid\_FlagCellsToBeRefinedForWeakScaling.C}
\end{itemize}




\subsection{{\tt AMRFLDSplit} Results}
\label{sec:AMRFLDSplit_results}

\begin{itemize}
\item AMRFLDSplit\_weak\_scaling
\item CosmoIonization\_q05z10\_AMRstatic
\item CosmoIonization\_q05z10\_AMRdynamic
\item CosmoIonization\_q05z4\_AMRstatic
\item CosmoIonization\_q05z4\_AMRdynamic
\item CosmoIonization\_q5z10\_AMRstatic
\item CosmoIonization\_q5z10\_AMRdynamic
\item CosmoIonization\_q5z4\_AMRstatic
\item CosmoIonization\_q5z4\_AMRdynamic
\item RHIonization1\_AMRdynamic
\item RHIonization1\_AMRstatic
\item RHIonization2\_AMRdynamic
\item RHIonization2\_AMRstatic
\end{itemize}





\section{Concluding remarks}
\label{sec:conclusions}

We wish to remark that the module is not large (one header
file, 15 C++ files, 6 F90 files), and all files begin with the 
{\tt AMRFLDSplit} prefix.  While we have strived to ensure that the
module is bug-free, there is still work to be done in enabling
additional physics, including fully implicit Helium/molecular
chemistry and more advanced time-stepping interactions with the rest
of Enzo (especially when ionization sources ``turn on'' abruptly).  

Feedback/suggestions to are welcome.


\bibliography{sources}
\bibliographystyle{siam}
\end{document}
